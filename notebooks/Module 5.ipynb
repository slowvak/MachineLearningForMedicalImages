{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiee evaluation metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAWMFLAIR</th>\n",
       "      <th>AIRT2</th>\n",
       "      <th>CSFFLAIR</th>\n",
       "      <th>AIRpost</th>\n",
       "      <th>TUMORT2</th>\n",
       "      <th>NAWMpost</th>\n",
       "      <th>GMpost</th>\n",
       "      <th>AIRpre</th>\n",
       "      <th>TUMORFLAIR</th>\n",
       "      <th>AIRFLAIR</th>\n",
       "      <th>TUMORpre</th>\n",
       "      <th>CSFpost</th>\n",
       "      <th>NAWMT2</th>\n",
       "      <th>GMT2</th>\n",
       "      <th>GMFLAIR</th>\n",
       "      <th>NAWMpre</th>\n",
       "      <th>TUMORpost</th>\n",
       "      <th>CSFT2</th>\n",
       "      <th>GMpre</th>\n",
       "      <th>CSFpre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>183.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>193.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>415.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NAWMFLAIR  AIRT2  CSFFLAIR  AIRpost  TUMORT2  NAWMpost  GMpost  AIRpre  \\\n",
       "0      191.0    0.0     140.0      0.0    251.0     258.0   256.0     0.0   \n",
       "1      183.0    0.0     137.0      0.0    235.0     255.0   277.0     0.0   \n",
       "2      185.0    0.0     141.0      0.0    226.0     252.0   280.0     0.0   \n",
       "3      193.0    0.0     158.0      0.0    221.0     250.0   267.0     0.0   \n",
       "4      200.0    0.0     163.0      0.0    199.0     251.0   244.0     0.0   \n",
       "\n",
       "   TUMORFLAIR  AIRFLAIR  TUMORpre  CSFpost  NAWMT2   GMT2  GMFLAIR  NAWMpre  \\\n",
       "0       173.0       0.0     398.0    320.0   313.0  240.0    131.0    354.0   \n",
       "1       146.0       0.0     387.0    309.0   316.0  243.0    124.0    366.0   \n",
       "2       137.0       0.0     376.0    304.0   332.0  250.0    124.0    378.0   \n",
       "3       123.0       0.0     369.0    306.0   367.0  239.0    132.0    364.0   \n",
       "4        98.0       0.0     377.0    310.0   364.0  249.0    145.0    366.0   \n",
       "\n",
       "   TUMORpost  CSFT2  GMpre  CSFpre  \n",
       "0      286.0  199.0  354.0   393.0  \n",
       "1      295.0  239.0  335.0   394.0  \n",
       "2      306.0  275.0  348.0   409.0  \n",
       "3      313.0  275.0  357.0   415.0  \n",
       "4      315.0  265.0  345.0   415.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "Data=pd.read_csv ('DataExample.csv')\n",
    "Data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ClassBrainTissuepost=(Data['ClassTissuePost'].values)\n",
    "ClassBrainTissuepost= (np.asarray(ClassBrainTissuepost))\n",
    "ClassBrainTissuepost=ClassBrainTissuepost[~np.isnan(ClassBrainTissuepost)]\n",
    "ClassBrainTissuepre=(Data[['ClassTissuePre']].values)\n",
    "ClassBrainTissuepre= (np.asarray(ClassBrainTissuepre))\n",
    "ClassBrainTissuepre=ClassBrainTissuepre[~np.isnan(ClassBrainTissuepre)]\n",
    "ClassTUMORpost=(Data[['ClassTumorPost']].values)\n",
    "ClassTUMORpost= (np.asarray(ClassTUMORpost))\n",
    "ClassTUMORpost=ClassTUMORpost[~np.isnan(ClassTUMORpost)]\n",
    "ClassTUMORpre=(Data[['ClassTumorPre']].values)\n",
    "ClassTUMORpre= (np.asarray(ClassTUMORpre))\n",
    "ClassTUMORpre=ClassTUMORpre[~np.isnan(ClassTUMORpre)]\n",
    "X_1 = np.stack((ClassBrainTissuepost,ClassBrainTissuepre)) # we only take the first two features.\n",
    "X_2 = np.stack((ClassTUMORpost,ClassTUMORpre))\n",
    "X=np.concatenate((X_1.transpose(), X_2.transpose()),axis=0)\n",
    "y =np.zeros((np.shape(X))[0])\n",
    "y[np.shape(X_1)[1]:]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a logistic regression model on the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make class predictions for the testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.897435897436\n"
     ]
    }
   ],
   "source": [
    "# Classification accuracy: percentage of correct predictions\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  3]\n",
      " [ 1 20]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save confusion matrix and slice into four pieces\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification Accuracy\n",
    "Overall, how often is the classifier correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.897435897436\n",
      "0.897435897436\n"
     ]
    }
   ],
   "source": [
    "print((TP + TN) / float(TP + TN + FP + FN))\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `incorrect` not found.\n"
     ]
    }
   ],
   "source": [
    "##  Classification Error\n",
    "Overall, how often is the classifier incorrect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Overall, how often is the classifier incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-63b187165557>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-63b187165557>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Overall, how often is the classifier incorrect\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Overall, how often is the classifier incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Overall, how often is the classifier incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print((FP + FN) / float(TP + TN + FP + FN))\n",
    "print(1 - metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity\n",
    "When the actual value is positive, how often is the prediction correct?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \n",
    "print(TP / float(TP + FN))\n",
    "print(metrics.recall_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity\n",
    "When the actual value is negative, how often is the prediction correct?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(TN / float(TN + FP))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Positive Rate\n",
    "\n",
    "When the actual value is negative, how often is the prediction incorrect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(FP / float(TN + FP))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "When a positive value is predicted, how often is the prediction correct?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(TP / float(TP + FP))\n",
    "print(metrics.precision_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves and Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_class)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for diabetes classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# define a function that accepts a threshold and prints sensitivity and specificity\n",
    "def evaluate_threshold(threshold):\n",
    "    print('Sensitivity:', tpr[thresholds > threshold][-1])\n",
    "    print('Specificity:', 1 - fpr[thresholds > threshold][-1])\n",
    "    \n",
    "evaluate_threshold(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC \n",
    "The percentage of the ROC plot that is underneath the curve:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(metrics.roc_auc_score(y_test, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate cross-validated AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
